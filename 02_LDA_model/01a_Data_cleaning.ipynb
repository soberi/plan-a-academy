{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "- Get dataframe of texts from Academy urls \n",
    "- Create test corpus to build cleaning function\n",
    "- Perform first round of data cleaning\n",
    "    - unwanted symbols\n",
    "    - make lowercase\n",
    "    - remove numbers\n",
    "- Second round of cleaning\n",
    "    - lemmatisation\n",
    "    - tokenization\n",
    "    - remove stop words\n",
    "- Third round of cleaning\n",
    "    - stemming\n",
    "    - create bigrams\n",
    "- BoW\n",
    "    \n",
    "### Once the cleaning steps are combined in a function, we'll run it on the full df of seperated content (i.e. content split by html elements) and full article df (i.e. non-split content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Academy texts dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../04_Data/processed_posts.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a subset as the corpus for testing\n",
    "\n",
    "For now we only need the url and content columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[['url','content']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning round 1\n",
    "Converting to lower case, get rid of punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_subset.loc[9].content\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_round1(text):\n",
    "    '''lowercase, remove punctuation, remove \\xa0, remove numbers + words with numbers'''\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('-', ' ', text)\n",
    "    \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[”“–‘’]', '', text )\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_round1 = cleaning_round1(test)\n",
    "test_round1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data cleaning round 2!\n",
    "\n",
    "The big guns are coming out: lemmatisation, tokenization, stopword removal.\n",
    "\n",
    "### 4.1 Using lemmatization to reduce words to their root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatisation\n",
    "\n",
    "def lemmatizer(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    text_out = []\n",
    "    tokens = nlp(text)\n",
    "    text_out = [\" \".join(token.lemma_ for token in tokens)]\n",
    "    \n",
    "    text_out = [re.sub('-PRON-', 'i', str(text)) for text in text_out]\n",
    "    \n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lem = lemmatizer(test_round1)\n",
    "test_lem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tokenizing and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    out = [[word for word in simple_preprocess(str(doc))\n",
    "            if word not in stop_words]\n",
    "            for doc in texts]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_no_stp = remove_stopwords(test_lem)\n",
    "test_no_stp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data cleaninfg round 3!\n",
    "\n",
    "Stemming and bigrams\n",
    "\n",
    "### 5.1 Stemming with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps  = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(content):\n",
    "    ps  = PorterStemmer()\n",
    "\n",
    "    stemmed = [ps.stem(w) for w in content]\n",
    "    \n",
    "    return stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_stem = stemmer(test_no_stp[0])\n",
    "test_stem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating combined NLP cleaner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step should be run first to update the df, so we can see how the\n",
    "# content has been transformed by the cleaning\n",
    "\n",
    "def nlp_cleaner(content):\n",
    "    text = cleaning_round1(content)\n",
    "    text = lemmatizer(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemmer(text[0])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_clean = nlp_cleaner(test)\n",
    "test_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Bigrams\n",
    "\n",
    "Making this part of the get corpus function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building bigram models\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    \n",
    "    return bigram_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# won't create bigrams for single text, but this is to test output\n",
    "\n",
    "test_bigram = bigrams(test_clean)\n",
    "print(test_bigram[test_clean])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get cleaned df\n",
    "Before creating the corpus, it would be helpful to have a cleaned df (inlc. bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "\n",
    "    df['content'] = df['content'].apply(nlp_cleaner)\n",
    "    df['content'] = df['content'].apply(lambda x:' '.join(x))\n",
    "    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = clean_df(df_subset)\n",
    "df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Get the corpus\n",
    "\n",
    "Combining all data-cleaning steps to create corpus and BoW for LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    \n",
    "    words = list((df.content))\n",
    "    words = [[word for word in nlp_cleaner(doc)]\n",
    "            for doc in words]\n",
    "    \n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram_set = [bigram_mod[article] for article in words]\n",
    "    \n",
    "    id2word = gensim.corpora.Dictionary(bigram_set)\n",
    "    id2word.compactify()\n",
    "    \n",
    "    corpus = [id2word.doc2bow(text) for text in bigram_set]\n",
    "\n",
    "    return corpus, id2word, bigram_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, id2word, train_bigram = get_corpus(df_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if texts are clean\n",
    "\n",
    "train_bigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Transforming DF for LDA model\n",
    "\n",
    "Running the entire df ('../04_Data/processed_posts.csv') through nlp_cleaner, then creating corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
