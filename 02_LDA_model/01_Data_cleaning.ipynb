{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "- Get dataframe of texts from Academy urls \n",
    "- Create test corpus to build cleaning function\n",
    "- Perform first round of data cleaning\n",
    "    - unwanted symbols\n",
    "    - make lowercase\n",
    "    - remove numbers\n",
    "- Second round of cleaning\n",
    "    - lemmatisation\n",
    "    - tokenization\n",
    "    - remove stop words\n",
    "- Third round of cleaning\n",
    "    - stemming\n",
    "    - create bigrams\n",
    "- BoW\n",
    "    \n",
    "### Once the cleaning steps are combined in a function, we'll run it on the full df of seperated content (i.e. content split by html elements) and full article df (i.e. non-split content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Academy texts dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing full df of content seperated by html element\n",
    "\n",
    "with open('../04_Data/academy_posts.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a subset as the corpus for testing\n",
    "\n",
    "For now we only need the url and content columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[['url','content']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning round 1\n",
    "Converting to lower case, get rid of punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_subset.loc[6].content\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_round1(text):\n",
    "    '''lowercase, remove punctuation, remove \\xa0, remove numbers + words with numbers'''\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('-', ' ', text)\n",
    "    \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[”“–‘’]', '', text )\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_round1 = cleaning_round1(test)\n",
    "test_round1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data cleaning round 2!\n",
    "\n",
    "The big guns are coming out: lemmatisation, tokenization, stopword removal.\n",
    "\n",
    "### 4.1 Using lemmatization to reduce words to their root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatisation\n",
    "\n",
    "def lemmatizer(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    text_out = []\n",
    "    tokens = nlp(text)\n",
    "    text_out = [\" \".join(token.lemma_ for token in tokens)]\n",
    "    \n",
    "    text_out = [re.sub('-PRON-', 'i', str(text)) for text in text_out]\n",
    "    \n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lem = lemmatizer(test_round1)\n",
    "test_lem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tokenizing and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    out = [[word for word in simple_preprocess(str(doc))\n",
    "            if word not in stop_words]\n",
    "            for doc in texts]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_no_stp = remove_stopwords(test_lem)\n",
    "test_no_stp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data cleaninfg round 3!\n",
    "\n",
    "Stemming and bigrams\n",
    "\n",
    "### 5.1 Stemming with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps  = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(content):\n",
    "    ps  = PorterStemmer()\n",
    "\n",
    "    stemmed = [ps.stem(w) for w in content]\n",
    "    \n",
    "    return stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_stem = stemmer(test_no_stp[0])\n",
    "test_stem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating combined NLP cleaner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step should be run first to update the df, so we can see how the\n",
    "# content has been transformed by the cleaning\n",
    "\n",
    "def nlp_cleaner(content):\n",
    "    text = cleaning_round1(content)\n",
    "    text = lemmatizer(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemmer(text[0])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_clean = nlp_cleaner(test)\n",
    "test_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Bigrams\n",
    "\n",
    "Making this part of the get corpus function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building bigram models\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    \n",
    "    return bigram_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['us', 'presid', 'elect', 'joe', 'biden', 'vow', 'rejoin', 'pari', 'agreement', 'first', 'day', 'offic', 'januari', 'us', 'back', 'track', 'signatori', 'unit', 'nation', 'framework', 'convent', 'climat', 'chang', 'ratifi', 'histor', 'treati']\n"
     ]
    }
   ],
   "source": [
    "# won't create bigrams for single text, but this is to test output\n",
    "\n",
    "test_bigram = bigrams(test_clean)\n",
    "print(test_bigram[test_clean])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get cleaned df\n",
    "Before creating the corpus, it would be helpful to have a cleaned df (inlc. bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "\n",
    "    df['content'] = df['content'].apply(nlp_cleaner)\n",
    "    df['content'] = df['content'].apply(lambda x:' '.join(x))\n",
    "    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>christma around corner unfortun year mani abl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>start statist carbon footprint cau end year pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>time offic christma parti quiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>find sustain christma parti would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>may merri forc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://plana.earth/academy/how-joe-biden-u-s-...</td>\n",
       "      <td>fifth anniversari pari climat agreement time g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://plana.earth/academy/how-joe-biden-u-s-...</td>\n",
       "      <td>presid elect joe biden vow rejoin pari agreeme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://plana.earth/academy/how-joe-biden-u-s-...</td>\n",
       "      <td>follow unit state withdraw pari climat agreeme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://plana.earth/academy/how-joe-biden-u-s-...</td>\n",
       "      <td>today trump administr offici leav pari climat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://plana.earth/academy/how-joe-biden-u-s-...</td>\n",
       "      <td>today trump administr offici leav pari climat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://plana.earth/academy/how-sustainable-is...   \n",
       "1  https://plana.earth/academy/how-sustainable-is...   \n",
       "2  https://plana.earth/academy/how-sustainable-is...   \n",
       "3  https://plana.earth/academy/how-sustainable-is...   \n",
       "4  https://plana.earth/academy/how-sustainable-is...   \n",
       "5  https://plana.earth/academy/how-joe-biden-u-s-...   \n",
       "6  https://plana.earth/academy/how-joe-biden-u-s-...   \n",
       "7  https://plana.earth/academy/how-joe-biden-u-s-...   \n",
       "8  https://plana.earth/academy/how-joe-biden-u-s-...   \n",
       "9  https://plana.earth/academy/how-joe-biden-u-s-...   \n",
       "\n",
       "                                             content  \n",
       "0  christma around corner unfortun year mani abl ...  \n",
       "1  start statist carbon footprint cau end year pa...  \n",
       "2                     time offic christma parti quiz  \n",
       "3                  find sustain christma parti would  \n",
       "4                                     may merri forc  \n",
       "5  fifth anniversari pari climat agreement time g...  \n",
       "6  presid elect joe biden vow rejoin pari agreeme...  \n",
       "7  follow unit state withdraw pari climat agreeme...  \n",
       "8  today trump administr offici leav pari climat ...  \n",
       "9  today trump administr offici leav pari climat ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = clean_df(df_subset)\n",
    "df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Get the corpus\n",
    "\n",
    "Combining all data-cleaning steps to create corpus and BoW for LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    \n",
    "    words = list((df.content))\n",
    "    words = [[word for word in nlp_cleaner(doc)]\n",
    "            for doc in words]\n",
    "    \n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram_set = [bigram_mod[article] for article in words]\n",
    "    \n",
    "    id2word = gensim.corpora.Dictionary(bigram_set)\n",
    "    id2word.compactify()\n",
    "    \n",
    "    corpus = [id2word.doc2bow(text) for text in bigram_set]\n",
    "\n",
    "    return corpus, id2word, bigram_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, id2word, train_bigram = get_corpus(df_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if texts are clean\n",
    "\n",
    "train_bigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Transforming success_urls for LDA model\n",
    "\n",
    "Using success_urls as it was previously tested. It already went through some cleaning steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../04_Data/success_g13.pkl', 'rb') as file:\n",
    "    success_g13 = pickle.load(file)\n",
    "    \n",
    "success_g13.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams are not part of the clean_df process\n",
    "\n",
    "# success_clean = clean_df(success_urls)\n",
    "# success_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 10 minutes to run\n",
    "\n",
    "succ_corpus, succ_id2word, succ_train_bigram = get_corpus(success_g13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succ_train_bigram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../04_Data/success_g13_corpus.pkl', 'wb') as sc:\n",
    "    pickle.dump(succ_corpus, sc, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../04_Data/success_g13_id2word.pkl', 'wb') as si:\n",
    "    pickle.dump(succ_id2word, si, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../04_Data/success_g13_train_bigram.pkl', 'wb') as st:\n",
    "    pickle.dump(succ_train_bigram, st, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Running full df through nlp_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>How sustainable is your office Christmas party?</td>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>Christmas is just around the corner! Unfortuna...</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>How sustainable is your office Christmas party?</td>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>Before we start, here are a few statistics on ...</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>How sustainable is your office Christmas party?</td>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>It is time for the office Christmas Party Quiz!</td>\n",
       "      <td>h2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>How sustainable is your office Christmas party?</td>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>Find out how sustainable your Christmas Party ...</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://plana.earth/academy/how-sustainable-is...</td>\n",
       "      <td>How sustainable is your office Christmas party?</td>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>May the Merry Force be with you!</td>\n",
       "      <td>h2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://plana.earth/academy/how-sustainable-is...   \n",
       "1  https://plana.earth/academy/how-sustainable-is...   \n",
       "2  https://plana.earth/academy/how-sustainable-is...   \n",
       "3  https://plana.earth/academy/how-sustainable-is...   \n",
       "4  https://plana.earth/academy/how-sustainable-is...   \n",
       "\n",
       "                                             title  published  \\\n",
       "0  How sustainable is your office Christmas party? 2020-12-18   \n",
       "1  How sustainable is your office Christmas party? 2020-12-18   \n",
       "2  How sustainable is your office Christmas party? 2020-12-18   \n",
       "3  How sustainable is your office Christmas party? 2020-12-18   \n",
       "4  How sustainable is your office Christmas party? 2020-12-18   \n",
       "\n",
       "                                             content tag  \n",
       "0  Christmas is just around the corner! Unfortuna...   p  \n",
       "1  Before we start, here are a few statistics on ...   p  \n",
       "2    It is time for the office Christmas Party Quiz!  h2  \n",
       "3  Find out how sustainable your Christmas Party ...   p  \n",
       "4                  May the Merry Force be with you!   h2  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about an hour to run\n",
    "\n",
    "# full_academy_corpus, full_academy_id2word, full_academy_train_bigram = get_corpus(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../04_Data/full_academy_corpus.pkl', 'wb') as fc:\n",
    "    pickle.dump(full_academy_corpus, fc, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../04_Data/full_academy_id2word.pkl', 'wb') as fi:\n",
    "    pickle.dump(full_academy_id2word, fi, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../04_Data/full_academy_train_bigram.pkl', 'wb') as ft:\n",
    "    pickle.dump(full_academy_train_bigram, ft, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
