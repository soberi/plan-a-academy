{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering and cleaning\n",
    "\n",
    "## Before running the LDA model, we'll need to get the data into a clean, useable format:\n",
    "\n",
    "- Get dataframe of texts from Academy urls \n",
    "- Create corpus(text must be combined per url, for now). df(url:combined_content)\n",
    "- Perform first round of data cleaning\n",
    "    - unwanted symbols\n",
    "    - make lowercase\n",
    "    - remove numbers\n",
    "- Second round of cleaning\n",
    "    - lemmatisation\n",
    "    - stemming\n",
    "    - remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Academy texts dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../04_Data/processed_posts.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create corpus\n",
    "\n",
    "We will use the initial setup (looking at specific html tags) as well as the per article analysis (which is more useful for EDA). For that we need to drop the 'tag', 'title' and 'published' column and combine rows with the same url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Corpus with articles separated by tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping important columns\n",
    "\n",
    "df_separated = df[['url','content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Everything looks good so far, let's just pick a random article\n",
    "# and check that everything is okay\n",
    "\n",
    "df_separated.loc[45].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl_cellar/corpus_separated.pkl', 'wb') as ds:\n",
    "    pickle.dump(df_separated, ds, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Corpus with articles as a whole\n",
    "This is mostly for EDA, to make sure the cleaned data offers valid results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining rows with the same url\n",
    "# Should only have 159 rows\n",
    "\n",
    "df_combined = df_separated.groupby(['url'])['content'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_combined.loc[45].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle and move on\n",
    "\n",
    "with open('pkl_cellar/corpus_combined.pkl', 'wb') as dc:\n",
    "    pickle.dump(df_combined, dc, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning round 1!\n",
    "\n",
    "Converting to lower case, get rid of punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_separated.loc[45].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_round1(text):\n",
    "    '''lowercase, remove punctuation, remove \\xa0, remove numbers + words with numbers'''\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('-', ' ', text)\n",
    "    \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[”“–‘’]', '', text )\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_round1 = cleaning_round1(test)\n",
    "test_round1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data cleaning round 2!\n",
    "\n",
    "The big guns are coming out: lemmatisation, tokenization, stopword removal. (perhaps bigrams and trigrams at later stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Original text\n",
    "\n",
    "'We have made some serious upgrades to help you measure more accurately your carbon footprint, get better insights, and enhance your decision making. We have spent a lot of time thinking about what we could do to make your journey to sustainability easier. Clarity, depth, simplicity, the three words that guided this work.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Using lemmatization to reduce words to their root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatisation\n",
    "\n",
    "def lemmatizer(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    text_out = []\n",
    "    tokens = nlp(text)\n",
    "    text_out = [\" \".join(token.lemma_ for token in tokens)]\n",
    "    \n",
    "    text_out = [re.sub('-PRON-', 'i', str(text)) for text in text_out]\n",
    "    \n",
    "    return \"\".join(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_lem = lemmatizer(test_round1)\n",
    "text_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tokenizing (separating words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize texts\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tkn = tokenize(text_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_tkn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Removing stop words from tokenized list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Takes tokenized (list) of words and removes stopwords'''\n",
    "    \n",
    "    stp_words = set(stopwords.words('english'))\n",
    "    \n",
    "    no_stp_words = [word for word in text if word not in stp_words]\n",
    "\n",
    "    return ' '.join(no_stp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_sw = remove_stopwords(text_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_no_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running all texts through cleaner\n",
    "\n",
    "- Step 1: Create one function to clean a test list\n",
    "- Step 2: Test it!\n",
    "- Step 3: Run entire set through cleaner\n",
    "- Step 4: Pickle it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_separated.loc[[6, 37, 28]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_cleaner(df):\n",
    "    \n",
    "    df['content'] = df['content'].apply(cleaning_round1)\n",
    "    \n",
    "    # cleaning round 2\n",
    "    df['content'] = df['content'].apply(lemmatizer)\n",
    "    df['content'] = df['content'].apply(tokenize)\n",
    "    df['content'] = df['content'].apply(remove_stopwords)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_cleaner(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Everything works, time to take the entire df_separated to the cleaners\n",
    "... then it's pickle time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs about an hour to run\n",
    "\n",
    "#lda_df = nlp_cleaner(df_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl_cellar/corpus_processed.pkl', 'rb') as file:\n",
    "    lda_df = pickle.load(file)\n",
    "\n",
    "lda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Now using nlp_cleaner( ) on df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 5 minutes to run\n",
    "\n",
    "lda_combined_articles = nlp_cleaner(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_combined_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pkl_cellar/corpus_processed_full_articles.pkl', 'wb') as fa:\n",
    "#    pickle.dump(lda_combined_articles, fa, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
